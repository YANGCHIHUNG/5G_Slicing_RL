"""
evaluate.py

æ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦è…³æœ¬ã€‚
1. è¼‰å…¥è¨“ç·´å¥½çš„æœ€ä½³æ¨¡å‹ (best_model.zip)ã€‚
2. åœ¨ç’°å¢ƒä¸­åŸ·è¡Œæ¨è«– (Inference)ã€‚
3. æ”¶é›†æ•¸æ“šä¸¦ç¹ªè£½åœ–è¡¨ (Results)ã€‚
"""

import os
import yaml
import numpy as np
import pandas as pd
from stable_baselines3 import SAC
from src.envs.slicing_env import NetworkSlicingEnv
from src.utils.plotter import plot_evaluation_results

# ==========================================
# è¨­å®šå€ (Configuration)
# ==========================================
# è«‹å°‡æ­¤è·¯å¾‘æ”¹ç‚ºæ‚¨å¯¦éš›è¨“ç·´å‡ºä¾†çš„å¯¦é©—è³‡æ–™å¤¾åç¨±
# ä¾‹å¦‚: "logs/sac_slicing_baseline_v1_20260121-120000"
EXPERIMENT_DIR = "logs/sac_slicing_baseline_v1_20260206-154124" 

# åƒæ•¸è¨­å®š
CONFIG_PATH = "configs/best_config_optuna.yaml"
MODEL_FILENAME = "best_model.zip" # å„ªå…ˆè®€å–æœ€ä½³æ¨¡å‹
# MODEL_FILENAME = "final_model.zip" # è‹¥æ²’æœ‰ best_model å‰‡è®€é€™å€‹

# æ¸¬è©¦é•·åº¦ (Steps)
EVAL_STEPS = 2000 # æ¸¬è©¦ 1 ç§’é˜ (2000 * 0.5ms)


def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)

def evaluate():
    print(f"--- Starting Evaluation ---")
    
    # 1. æª¢æŸ¥è·¯å¾‘
    # å¦‚æœä½¿ç”¨è€…é‚„æ²’æ”¹ EXPERIMENT_DIRï¼Œå˜—è©¦è‡ªå‹•å°‹æ‰¾æœ€æ–°çš„ log
    global EXPERIMENT_DIR
    if "YOUR_TIMESTAMP_HERE" in EXPERIMENT_DIR:
        if os.path.exists("models"):
            # æ‰¾ models è³‡æ–™å¤¾è£¡æœ€æ–°çš„è³‡æ–™å¤¾
            dirs = [os.path.join("models", d) for d in os.listdir("models") if os.path.isdir(os.path.join("models", d))]
            if dirs:
                EXPERIMENT_DIR = max(dirs, key=os.path.getmtime)
                print(f"âš ï¸ Auto-detected latest experiment: {EXPERIMENT_DIR}")
            else:
                print("âŒ No models found. Please train first!")
                return
        else:
            print("âŒ 'models' directory not found.")
            return

    model_path = os.path.join(EXPERIMENT_DIR, MODEL_FILENAME)
    if not os.path.exists(model_path):
        # å˜—è©¦å» models/ è³‡æ–™å¤¾æ‰¾ (å› ç‚º main.py å­˜æª”é‚è¼¯å¯èƒ½åˆ†é–‹)
        # é€™è£¡åšä¸€å€‹å®¹éŒ¯è™•ç†
        alt_path = model_path.replace("logs", "models")
        if os.path.exists(alt_path):
            model_path = alt_path
        else:
            print(f"âŒ Model not found at: {model_path}")
            return

    print(f"ğŸ“‚ Loading Model: {model_path}")

    # 2. å»ºç«‹ç’°å¢ƒ
    config = load_config(CONFIG_PATH)
    env = NetworkSlicingEnv(config)
    
    # 3. è¼‰å…¥æ¨¡å‹
    model = SAC.load(model_path, env=env)
    
    # 4. åŸ·è¡Œæ¨¡æ“¬è¿´åœˆ
    print(f"â–¶ï¸ Running simulation for {EVAL_STEPS} steps...")
    
    obs, _ = env.reset()
    history = []
    
    for step in range(EVAL_STEPS):
        # deterministic=True ä»£è¡¨é—œé–‰éš¨æ©Ÿæ¢ç´¢ (Exploration)ï¼Œåªä½¿ç”¨å­¸åˆ°çš„æœ€ä½³ç­–ç•¥
        action, _ = model.predict(obs, deterministic=True)
        
        obs, reward, terminated, truncated, info = env.step(action)
        
        # æ”¶é›†æ•¸æ“š
        history.append(info)
        
        if terminated or truncated:
            obs, _ = env.reset()

    # 5. æ•¸æ“šè™•ç†
    df = pd.DataFrame(history)
    
    # è¨ˆç®—é—œéµæŒ‡æ¨™
    avg_embb = df['throughput_embb_mbps'].mean()
    avg_urllc = df['throughput_urllc_mbps'].mean()
    violation_rate = (df['dropped_urllc'] > 0).mean() * 100
    
    print("\n--- ğŸ“Š Evaluation Summary ---")
    print(f"Avg eMBB Throughput : {avg_embb:.2f} Mbps")
    print(f"Avg URLLC Throughput: {avg_urllc:.2f} Mbps")
    print(f"URLLC Violation Rate: {violation_rate:.2f}%") # ç›®æ¨™æ‡‰è©²è¦æ˜¯ 0%
    
    # 6. ç¹ªåœ–
    results_dir = os.path.join("results", os.path.basename(EXPERIMENT_DIR))
    plot_evaluation_results(df, results_dir)
    
    # å„²å­˜åŸå§‹æ•¸æ“š CSV ä»¥ä¾¿å¾ŒçºŒåˆ†æ
    csv_path = os.path.join(results_dir, "eval_data.csv")
    df.to_csv(csv_path, index=False)
    print(f"ğŸ’¾ Data saved to: {csv_path}")

if __name__ == "__main__":
    evaluate()