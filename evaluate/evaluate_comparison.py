"""
evaluate_comparison.py

æ•™å¸«æ¨¡å‹èˆ‡å­¸ç”Ÿæ¨¡å‹å…¨æ–¹ä½è©•ä¼°èˆ‡æ¯”è¼ƒè…³æœ¬ã€‚
Comprehensive Evaluation Script for Teacher vs. Student Models.

åŠŸèƒ½ï¼š
1. éœæ…‹åˆ†æï¼šæ¯”è¼ƒåƒæ•¸é‡ (Model Size) èˆ‡ å£“ç¸®ç‡ (Compression Ratio)ã€‚
2. å‹•æ…‹æ•ˆèƒ½ï¼šåˆ†åˆ¥åŸ·è¡Œ Teacher èˆ‡ Studentï¼Œæ¯”è¼ƒååé‡ (Throughput) èˆ‡ QoS æŒ‡æ¨™ã€‚
3. è’¸é¤¾å“è³ªï¼šåœ¨å­¸ç”ŸåŸ·è¡ŒæœŸé–“ï¼Œè¨ˆç®—èˆ‡è€å¸«çš„å‹•ä½œå·®ç•° (Fidelity/MSE)ã€‚
4. é‹ç®—æ•ˆç‡ï¼šæ¸¬é‡æ¨è«–å»¶é² (Inference Latency) èˆ‡ åŠ é€Ÿå€ç‡ (Speedup)ã€‚
"""

import os
import time
import yaml
import numpy as np
import pandas as pd
import torch
from stable_baselines3 import SAC
from src.envs.slicing_env import NetworkSlicingEnv
from src.utils.plotter import plot_evaluation_results

# ==========================================
# åƒæ•¸è¨­å®šå€ (Configuration)
# ==========================================
# è¨­å®šæª”è·¯å¾‘ (è«‹ç¢ºä¿èˆ‡è¨“ç·´æ™‚ä¸€è‡´)
CONFIG_PATH = "configs/default_config.yaml"

# æ¨¡å‹è·¯å¾‘ (è«‹ä¿®æ”¹ç‚ºæ‚¨å¯¦éš›çš„ .zip æª”æ¡ˆè·¯å¾‘)
# ç¯„ä¾‹ï¼š "logs/sac_teacher_v1/best_model.zip"
TEACHER_MODEL_PATH = "logs/(Light Load)sac_slicing_baseline_v1_20260206-154124/best_model.zip" 

# ç¯„ä¾‹ï¼š "logs/distilled_student_v1/best_model.zip"
STUDENT_MODEL_PATH = "models/distilled_sac_student_v1_20260207-172248/best_model.zip"

# è©•ä¼°æ­¥æ•¸ (2000 steps = 1ç§’é˜æ¨¡æ“¬æ™‚é–“)
EVAL_STEPS = 2000

# ==========================================
# å·¥å…·å‡½å¼
# ==========================================

def load_config(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Config file not found: {path}")
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def count_parameters(model):
    """è¨ˆç®—æ¨¡å‹ç­–ç•¥ç¶²è·¯ (Policy Network) çš„å¯è¨“ç·´åƒæ•¸é‡"""
    if not model.policy:
        return 0
    return sum(p.numel() for p in model.policy.parameters() if p.requires_grad)

def benchmark_pure_inference(model, steps=10000):
    """
    æ¥µé™æ¨è«–æ¸¬è©¦ï¼šæ’é™¤æ‰€æœ‰æ•¸æ“šè½‰æ›ï¼Œåªæ¸¬é‡ç¥ç¶“ç¶²è·¯ forward æ™‚é–“
    """
    import torch
    import time
    
    # å»ºç«‹ä¸€å€‹å›ºå®šçš„ Dummy Input (Batch Size = 1, Obs Dim = 6)
    # å‡è¨­ observation space ç¶­åº¦æ˜¯ 6ï¼Œè«‹æ ¹æ“šæ‚¨çš„ç’°å¢ƒèª¿æ•´
    obs_dim = model.observation_space.shape[0]
    dummy_input = torch.randn(1, obs_dim, device=model.device)
    
    # é ç†± (Warmup)
    for _ in range(100):
        with torch.no_grad():
            model.policy.forward(dummy_input)
            
    # é–‹å§‹è¨ˆæ™‚
    start = time.perf_counter()
    with torch.no_grad():
        for _ in range(steps):
            model.policy.forward(dummy_input)
    end = time.perf_counter()
    
    avg_time_ms = ((end - start) / steps) * 1000
    return avg_time_ms

def run_simulation(model, env, steps, name="Model", teacher_model=None):
    print(f"â–¶ï¸ Running simulation for {name} ({steps} steps)...")
    
    obs, _ = env.reset()
    history = []
    inference_times = []
    action_diffs = []
    
    # [æ–°å¢] é å…ˆæº–å‚™ PyTorch Tensor æ ¼å¼çš„ observationï¼Œé¿å…æ¸¬é‡åˆ° numpy->tensor çš„è½‰æ›æ™‚é–“
    # æ³¨æ„ï¼šé€™åªæ˜¯ç‚ºäº†æ¸¬é‡ç´”æ¨è«–é€Ÿåº¦çš„æ¥µé™å€¼
    dummy_obs_tensor = torch.as_tensor(obs, device=model.device).unsqueeze(0)

    # é ç†± (Warm-up)
    _ = model.policy.forward(dummy_obs_tensor, deterministic=True)

    for _ in range(steps):
        # ============================================
        # [ä¿®æ­£] æ¸¬é‡ Raw PyTorch æ¨è«–æ™‚é–“ (ç¹é SB3 overhead)
        # ============================================
        obs_tensor = torch.as_tensor(obs, device=model.device).unsqueeze(0)
        
        start_t = time.perf_counter()
        with torch.no_grad():
            # ç›´æ¥å‘¼å« Policy ç¶²è·¯é€²è¡Œæ¨è«–
            action_tensor = model.policy.forward(obs_tensor, deterministic=True)
        end_t = time.perf_counter()
        
        # è½‰æ›å› numpy çµ¦ç’°å¢ƒä½¿ç”¨ (é€™æ®µä¸è¨ˆå…¥æ¨è«–æ™‚é–“ï¼Œå› ç‚ºå¯¦å‹™ä¸Šæ˜¯åœ¨ C++ ç«¯è™•ç†)
        action = action_tensor.cpu().numpy()[0]
        
        inference_times.append((end_t - start_t) * 1000.0) # è½‰ ms
        # ============================================

        # 2. (é¸å¡«) å½±å­æ¨¡å¼ï¼šæ¸¬é‡è€å¸«å‹•ä½œä»¥è¨ˆç®—å·®ç•°
        if teacher_model:
            # è€å¸«ä¹Ÿç”¨åŒæ¨£æ–¹å¼å–å¾—å‹•ä½œä»¥æ±‚å…¬å¹³ï¼Œè¨ˆç®— MSE
            action_teacher, _ = teacher_model.predict(obs, deterministic=True)
            diff = np.mean((action - action_teacher)**2)
            action_diffs.append(diff)

        # 3. ç’°å¢ƒäº’å‹•
        obs, reward, terminated, truncated, info = env.step(action)
        history.append(info)

        if terminated or truncated:
            obs, _ = env.reset()
            
    df = pd.DataFrame(history)
    avg_inference_time = np.mean(inference_times)
    fidelity_score = np.mean(action_diffs) if action_diffs else None
    
    return df, avg_inference_time, fidelity_score

def print_metrics(df, label):
    """è¨ˆç®—ä¸¦åˆ—å°é—œéµ QoS æŒ‡æ¨™"""
    avg_embb = df['throughput_embb_mbps'].mean()
    avg_urllc = df['throughput_urllc_mbps'].mean()
    
    # è¨ˆç®—å»¶é²çµ±è¨ˆ (éæ¿¾æ‰ç„¡æ•¸æ“šçš„ 0 å€¼)
    latencies = df[df['latency_urllc'] > 0]['latency_urllc'] * 1000 # è½‰ ms
    if len(latencies) > 0:
        avg_lat = latencies.mean()
        p99_lat = np.percentile(latencies, 99)
        max_lat = latencies.max()
    else:
        avg_lat = p99_lat = max_lat = 0.0

    violation_rate = (df['dropped_urllc'] > 0).mean() * 100
    
    print(f"--- {label} Metrics ---")
    print(f"   Throughput (Mbps) : eMBB {avg_embb:6.2f} | URLLC {avg_urllc:6.2f}")
    print(f"   URLLC Latency (ms): Avg  {avg_lat:6.3f} | P99   {p99_lat:6.3f} | Max {max_lat:6.3f}")
    print(f"   Violation Rate    : {violation_rate:6.2f}%")
    return avg_lat, violation_rate

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
def evaluate_comparison():
    print("="*60)
    print("ğŸ”¬ Teacher-Student Model Comprehensive Evaluation")
    print("="*60)

    # 1. æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(TEACHER_MODEL_PATH):
        print(f"âŒ Teacher model not found: {TEACHER_MODEL_PATH}")
        return
    if not os.path.exists(STUDENT_MODEL_PATH):
        print(f"âŒ Student model not found: {STUDENT_MODEL_PATH}")
        return

    # 2. è¼‰å…¥ç’°å¢ƒèˆ‡æ¨¡å‹
    print("\n[1/4] Loading Environment and Models...")
    config = load_config(CONFIG_PATH)
    env = NetworkSlicingEnv(config) # ä½¿ç”¨åŒä¸€ä»½ Config ç¢ºä¿å…¬å¹³

    print(f"   Loading Teacher: {TEACHER_MODEL_PATH}")
    teacher_model = SAC.load(TEACHER_MODEL_PATH, env=env)
    
    print(f"   Loading Student: {STUDENT_MODEL_PATH}")
    student_model = SAC.load(STUDENT_MODEL_PATH, env=env)

    # 3. éœæ…‹åˆ†æï¼šæ¨¡å‹å¤§å°
    print("\n[2/4] Static Analysis: Model Complexity")
    n_params_t = count_parameters(teacher_model)
    n_params_s = count_parameters(student_model)
    compression_ratio = n_params_t / n_params_s if n_params_s > 0 else 0
    
    print(f"   Teacher Params    : {n_params_t:,}")
    print(f"   Student Params    : {n_params_s:,}")
    print(f"   ğŸš€ Compression Ratio: {compression_ratio:.2f}x smaller")

    print("\n[2.5/4] Benchmarking Pure Inference Speed (CPU/GPU Raw Performance)...")
    raw_time_t = benchmark_pure_inference(teacher_model)
    raw_time_s = benchmark_pure_inference(student_model)
    
    raw_speedup = raw_time_t / raw_time_s
    
    print(f"   Teacher Pure Compute: {raw_time_t:.5f} ms")
    print(f"   Student Pure Compute: {raw_time_s:.5f} ms")
    print(f"   âš¡ True Speedup      : {raw_speedup:.2f}x")

    # 4. Phase 1: åŸ·è¡Œæ•™å¸«æ¨¡å‹ (å»ºç«‹åŸºæº–)
    print("\n[3/4] Phase 1: Evaluating Teacher Baseline...")
    df_t, time_t, _ = run_simulation(teacher_model, env, EVAL_STEPS, name="Teacher")
    _, vio_t = print_metrics(df_t, "Teacher")

    # 5. Phase 2: åŸ·è¡Œå­¸ç”Ÿæ¨¡å‹ (ä¸¦è¨ˆç®—å·®ç•°)
    print("\n[4/4] Phase 2: Evaluating Student Distillation...")
    # é€™è£¡å‚³å…¥ teacher_model æ˜¯ç‚ºäº†è¨ˆç®— "Fidelity" (å‹•ä½œå·®ç•°)ï¼Œä½†ç’°å¢ƒæ˜¯ç”±å­¸ç”Ÿæ§åˆ¶
    df_s, time_s, mse_val = run_simulation(student_model, env, EVAL_STEPS, name="Student", teacher_model=teacher_model)
    _, vio_s = print_metrics(df_s, "Student")

    # ==========================================
    # 6. æœ€çµ‚è©•ä¼°å ±å‘Š
    # ==========================================
    speedup = time_t / time_s if time_s > 0 else 0
    
    print("\n" + "="*60)
    print("ğŸ“Š FINAL COMPARISON REPORT")
    print("="*60)
    
    print(f"1. Efficiency (Speed & Size)")
    print(f"   - Model Size      : Reduced by {compression_ratio:.1f}x")
    print(f"   - Inference Time  : Teacher {time_t:.3f} ms vs Student {time_s:.3f} ms")
    print(f"   - Speedup         : {speedup:.2f}x faster âš¡")
    print(f"   - Real-time Check : {'âœ… PASS (<0.5ms)' if time_s < 0.5 else 'âŒ FAIL (>0.5ms)'}")

    print(f"\n2. Fidelity (Imitation Quality)")
    print(f"   - Action MSE      : {mse_val:.6f} (Lower is better)")
    
    print(f"\n3. Performance Qualification (QoS)")
    # ç°¡å–®çš„åˆæ ¼åˆ¤å®šé‚è¼¯
    is_qualified = True
    reasons = []
    
    if vio_s > 1.0: 
        is_qualified = False
        reasons.append(f"Violation Rate too high ({vio_s:.2f}%)")
    
    if time_s > 0.5:
        is_qualified = False
        reasons.append(f"Inference too slow ({time_s:.3f}ms)")
        
    print(f"   - Teacher Violation: {vio_t:.2f}%")
    print(f"   - Student Violation: {vio_s:.2f}%")
    print(f"   - Status           : {'âœ… QUALIFIED' if is_qualified else 'âŒ FAILED'}")
    
    if not is_qualified:
        print(f"   - Issues           : {', '.join(reasons)}")

    print("="*60)
    
    # å„²å­˜çµæœ
    results_dir = "results/comparison"
    os.makedirs(results_dir, exist_ok=True)
    df_t.to_csv(os.path.join(results_dir, "eval_teacher.csv"), index=False)
    df_s.to_csv(os.path.join(results_dir, "eval_student.csv"), index=False)
    print(f"\nğŸ’¾ Detailed logs saved to: {results_dir}")

if __name__ == "__main__":
    evaluate_comparison()