"""
evaluate_native_vs_distilled.py

æ¨¡å‹å°æ±ºè©•ä¼°è…³æœ¬ (Model Showdown Evaluation).
ç›´æ¥æ¯”è¼ƒå…©å€‹å·²è¨“ç·´å¥½çš„å°æ¨¡å‹ï¼š
1. Native Student: é€é main.py è¨“ç·´çš„ç´” SAC å°æ¨¡å‹ã€‚
2. Distilled Student: é€é train_distillation.py è¨“ç·´çš„è’¸é¤¾å°æ¨¡å‹ã€‚

ç›®çš„ï¼šé©—è­‰åœ¨ã€Œåƒæ•¸é‡ç›¸åŒã€çš„æƒ…æ³ä¸‹ï¼Œç¶“éè’¸é¤¾çš„æ¨¡å‹æ˜¯å¦åœ¨ QoS (å»¶é²/é•è¦ç‡) ä¸Šå„ªæ–¼åŸç”Ÿæ¨¡å‹ã€‚
"""

import os
import time
import yaml
import numpy as np
import pandas as pd
from stable_baselines3 import SAC
from src.envs.slicing_env import NetworkSlicingEnv

# ==========================================
# 1. åƒæ•¸èˆ‡è·¯å¾‘è¨­å®š (è«‹ä¿®æ”¹é€™è£¡)
# ==========================================
CONFIG_PATH = "configs/distillation_config.yaml"  # ç¢ºä¿ç’°å¢ƒè¨­å®šä¸€è‡´

# [Native Student] è·¯å¾‘ (æ‚¨ç”¨ main.py è¨“ç·´å‡ºä¾†çš„å°æ¨¡å‹)
# è«‹ä¿®æ”¹ç‚ºå¯¦éš›è·¯å¾‘ï¼Œä¾‹å¦‚ "logs/sac_native_small_v1/best_model.zip"
NATIVE_MODEL_PATH = "logs/small_model_sac_slicing_baseline_v1_20260210-214644/best_model.zip" 

# [Distilled Student] è·¯å¾‘ (æ‚¨ç”¨ train_distillation.py è¨“ç·´å‡ºä¾†çš„æ¨¡å‹)
# è«‹ä¿®æ”¹ç‚ºå¯¦éš›è·¯å¾‘
DISTILLED_MODEL_PATH = "models/distilled_sac_student_v1_20260207-172248/best_model.zip"

# è©•ä¼°æ­¥æ•¸ (2000 steps = 1ç§’æ¨¡æ“¬æ™‚é–“)
EVAL_STEPS = 5000 

# ==========================================
# 2. å·¥å…·å‡½å¼
# ==========================================
def load_config(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Config file not found: {path}")
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def run_evaluation(model, env, steps, name):
    """åŸ·è¡Œå–®ä¸€æ¨¡å‹çš„è©•ä¼°è¿´åœˆ"""
    print(f"â–¶ï¸ Running simulation for {name} ({steps} steps)...")
    
    obs, _ = env.reset()
    history = []
    
    # é ç†±
    _ = model.predict(obs, deterministic=True)
    
    start_time = time.time()
    for _ in range(steps):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        history.append(info)
        
        if terminated or truncated:
            obs, _ = env.reset()
    
    duration = time.time() - start_time
    print(f"   Done in {duration:.2f}s.")
    
    return pd.DataFrame(history)

def calculate_metrics(df):
    """è¨ˆç®—é—œéµæŒ‡æ¨™"""
    # ååé‡
    avg_embb = df['throughput_embb_mbps'].mean()
    avg_urllc = df['throughput_urllc_mbps'].mean()
    
    # å»¶é² (åªè¨ˆç®—æœ‰æ•¸æ“šçš„ TTI)
    latencies = df[df['latency_urllc'] > 0]['latency_urllc'] * 1000 # ms
    if len(latencies) > 0:
        avg_lat = latencies.mean()
        p99_lat = np.percentile(latencies, 99)
        max_lat = latencies.max()
    else:
        avg_lat = p99_lat = max_lat = 0.0
        
    # é•è¦ç‡
    violation_rate = (df['dropped_urllc'] > 0).mean() * 100
    
    return {
        "eMBB (Mbps)": avg_embb,
        "URLLC (Mbps)": avg_urllc,
        "Avg Latency (ms)": avg_lat,
        "P99 Latency (ms)": p99_lat,
        "Max Latency (ms)": max_lat,
        "Violation Rate (%)": violation_rate
    }

# ==========================================
# 3. ä¸»ç¨‹å¼
# ==========================================
def main():
    print("="*60)
    print("ğŸ¥Š Native vs. Distilled Student: The Showdown")
    print("="*60)
    
    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(NATIVE_MODEL_PATH):
        print(f"âŒ Native model not found: {NATIVE_MODEL_PATH}")
        print("   Tip: Update 'NATIVE_MODEL_PATH' in the script.")
        return
    if not os.path.exists(DISTILLED_MODEL_PATH):
        print(f"âŒ Distilled model not found: {DISTILLED_MODEL_PATH}")
        print("   Tip: Update 'DISTILLED_MODEL_PATH' in the script.")
        return

    # è¼‰å…¥ç’°å¢ƒ
    print(f"Loading Config: {CONFIG_PATH}")
    config = load_config(CONFIG_PATH)
    
    # ç¢ºä¿æµé‡è² è¼‰è¶³å¤ å¤§ï¼Œæ‰èƒ½çœ‹å‡ºå·®ç•° (å»ºè­° eMBB > 350)
    print(f"Traffic Settings: eMBB={config['traffic']['embb_arrival_rate_mbps']} Mbps, URLLC={config['traffic']['urllc_arrival_rate_mbps']} Mbps")
    
    env = NetworkSlicingEnv(config)
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nLoading Native Student: {NATIVE_MODEL_PATH}")
    native_model = SAC.load(NATIVE_MODEL_PATH, env=env)
    
    print(f"Loading Distilled Student: {DISTILLED_MODEL_PATH}")
    distilled_model = SAC.load(DISTILLED_MODEL_PATH, env=env)
    
    # åŸ·è¡Œè©•ä¼°
    print("\n" + "-"*30)
    df_native = run_evaluation(native_model, env, EVAL_STEPS, "Native Student")
    
    print("-" * 30)
    # é‡ç½®ç’°å¢ƒç¨®å­ä»¥ç¢ºä¿å…¬å¹³ (å¦‚æœ env æ”¯æ´ seed)
    # env.reset(seed=config['random_seed']) 
    df_distilled = run_evaluation(distilled_model, env, EVAL_STEPS, "Distilled Student")
    
    # è¨ˆç®—æŒ‡æ¨™
    metrics_native = calculate_metrics(df_native)
    metrics_distilled = calculate_metrics(df_distilled)
    
    # ==========================================
    # 4. è¼¸å‡ºæ¯”è¼ƒå ±è¡¨
    # ==========================================
    print("\n" + "="*60)
    print("ğŸ“Š Final Comparison Results")
    print("="*60)
    
    # å»ºç«‹æ¯”è¼ƒ DataFrame
    comp_df = pd.DataFrame([metrics_native, metrics_distilled], index=["Native", "Distilled"])
    
    # æ ¼å¼åŒ–è¼¸å‡º
    print(comp_df.round(4).to_string())
    
    print("\n" + "="*60)
    print("ğŸ† Verdict (Analysis):")
    
    # è‡ªå‹•åˆ¤è®€
    native_vio = metrics_native['Violation Rate (%)']
    distilled_vio = metrics_distilled['Violation Rate (%)']
    native_p99 = metrics_native['P99 Latency (ms)']
    distilled_p99 = metrics_distilled['P99 Latency (ms)']
    
    if distilled_vio < native_vio:
        print(f"âœ… Distilled model has LOWER Violation Rate ({distilled_vio:.2f}% vs {native_vio:.2f}%).")
        print("   -> Knowledge Distillation improved reliability!")
    elif distilled_vio == native_vio:
        if distilled_p99 < native_p99:
            print(f"âœ… Distilled model has LOWER P99 Latency ({distilled_p99:.3f}ms vs {native_p99:.3f}ms).")
            print("   -> Knowledge Distillation improved tail latency!")
        else:
             print(f"âš–ï¸ Performance is similar. (Diff: P99 {distilled_p99 - native_p99:.3f}ms)")
    else:
        print(f"âŒ Native model performed better. ({native_vio:.2f}% vs {distilled_vio:.2f}%)")
        print("   -> Check if the Teacher model was actually good, or if 'distillation_alpha' needs tuning.")

    # å­˜æª”
    results_dir = "results/native_vs_distilled"
    os.makedirs(results_dir, exist_ok=True)
    comp_df.to_csv(os.path.join(results_dir, "comparison_report.csv"))
    print(f"\nğŸ’¾ Report saved to: {results_dir}/comparison_report.csv")

if __name__ == "__main__":
    main()