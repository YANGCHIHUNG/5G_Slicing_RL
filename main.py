"""
main.py

5G ç¶²è·¯åˆ‡ç‰‡å¼·åŒ–å­¸ç¿’è¨“ç·´ä¸»ç¨‹å¼ (Training Entry Point)ã€‚
è² è²¬è®€å–è¨­å®šã€å»ºç«‹ç’°å¢ƒã€åˆå§‹åŒ– SAC Agentï¼Œä¸¦åŸ·è¡Œè¨“ç·´è¿´åœˆã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    # å–®åŸ·è¡Œç·’æ¸¬è©¦
    python -m main

    # ä½¿ç”¨ nohup åœ¨èƒŒæ™¯åŸ·è¡Œ
    nohup python -m main > main_202602051621.log 2>&1 &

    # æŸ¥çœ‹èƒŒæ™¯åŸ·è¡Œç‹€æ…‹
    ps aux | grep python | grep main

    # åœæ­¢èƒŒæ™¯åŸ·è¡Œ
    kill <PID>
"""

import os
import yaml
import time
import numpy as np
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback, CallbackList
from src.envs.slicing_env import NetworkSlicingEnv

def load_config(config_path="configs/best_config_optuna.yaml"):
    """è®€å– YAML è¨­å®šæª”"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found at: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


class DetailedLoggingCallback(BaseCallback):
    """
    è‡ªå®šç¾© Callbackï¼šæ¯éš”ä¸€å®šæ­¥æ•¸é¡¯ç¤ºè©³ç´°çš„è¨“ç·´ç‹€æ…‹
    åŒ…æ‹¬ç¶²è·¯åˆ‡ç‰‡åˆ†é…æ¯”ä¾‹ã€Buffer ç‹€æ³ã€ååé‡ã€å»¶é²ç­‰
    """
    def __init__(self, log_freq=100, verbose=0):
        """
        Args:
            log_freq (int): æ¯éš”å¤šå°‘ timesteps åˆ—å°ä¸€æ¬¡
            verbose (int): è©³ç´°ç¨‹åº¦ (0=ç°¡æ½”, 1=è©³ç´°)
        """
        super(DetailedLoggingCallback, self).__init__(verbose)
        self.log_freq = log_freq
        self.episode_rewards = []
        self.episode_lengths = []
        self.current_episode_reward = 0
        self.current_episode_length = 0
        
    def _on_step(self) -> bool:
        """
        æ¯å€‹ step å¾Œè¢«å‘¼å«
        """
        # ç´¯ç©ç•¶å‰ episode çš„çå‹µ
        self.current_episode_reward += self.locals['rewards'][0]
        self.current_episode_length += 1
        
        # æª¢æŸ¥ episode æ˜¯å¦çµæŸ
        if self.locals['dones'][0]:
            self.episode_rewards.append(self.current_episode_reward)
            self.episode_lengths.append(self.current_episode_length)
            self.current_episode_reward = 0
            self.current_episode_length = 0
        
        # æ¯éš” log_freq æ­¥é©Ÿåˆ—å°è©³ç´°è³‡è¨Š
        if self.num_timesteps % self.log_freq == 0:
            self._print_detailed_info()
            
        return True
    
    def _print_detailed_info(self):
        """åˆ—å°è©³ç´°çš„è¨“ç·´ç‹€æ…‹"""
        # å¾ locals ä¸­ç²å–æœ€æ–°çš„è³‡è¨Š
        infos = self.locals.get('infos', [{}])
        if len(infos) > 0:
            info = infos[0]
        else:
            return
        
        # ç²å–ç•¶å‰ observation (ç‹€æ…‹)
        obs = self.locals.get('new_obs', None)
        if obs is not None and len(obs) > 0:
            obs = obs[0]  # [eMBB_Load, eMBB_HoL, URLLC_Load, URLLC_HoL, CQI_eMBB, CQI_URLLC]
        
        # ç²å–ç•¶å‰ action (å‹•ä½œ)
        actions = self.locals.get('actions', None)
        if actions is not None and len(actions) > 0:
            action = actions[0]  # [w_embb, w_urllc]
        else:
            action = [0, 0]

        # è‹¥ç’°å¢ƒæœ‰å¥—ç”¨ç´„æŸï¼Œå„ªå…ˆé¡¯ç¤ºå¯¦éš›å¥—ç”¨çš„æ¬Šé‡
        action_applied = info.get('action_applied', action)
        
        print("\n" + "="*80)
        print(f"ğŸ“Š Training Status at Step {self.num_timesteps}")
        print("="*80)
        
        # --- 1. Episode çµ±è¨ˆ ---
        if len(self.episode_rewards) > 0:
            recent_rewards = self.episode_rewards[-10:] if len(self.episode_rewards) >= 10 else self.episode_rewards
            avg_reward = np.mean(recent_rewards)
            avg_length = np.mean(self.episode_lengths[-10:]) if len(self.episode_lengths) >= 10 else np.mean(self.episode_lengths)
            print(f"ğŸ“ˆ Episodes completed: {len(self.episode_rewards)}")
            print(f"   Avg Reward (last 10): {avg_reward:.2f}")
            print(f"   Avg Length (last 10): {avg_length:.1f}")
        
        # --- 2. ç¶²è·¯åˆ‡ç‰‡è³‡æºåˆ†é… ---
        print(f"\nğŸ”§ Resource Allocation:")
        total_w = action_applied[0] + action_applied[1] + 1e-9
        w_embb_norm = action_applied[0] / total_w
        w_urllc_norm = action_applied[1] / total_w
        print(f"   eMBB Weight:  {action_applied[0]:.4f} (normalized: {w_embb_norm:.1%})")
        print(f"   URLLC Weight: {action_applied[1]:.4f} (normalized: {w_urllc_norm:.1%})")
        
        if 'rbs_embb' in info and 'rbs_urllc' in info:
            total_rbs = info['rbs_embb'] + info['rbs_urllc']
            print(f"   eMBB RBs:  {info['rbs_embb']:3d} / {total_rbs} ({info['rbs_embb']/total_rbs:.1%})")
            print(f"   URLLC RBs: {info['rbs_urllc']:3d} / {total_rbs} ({info['rbs_urllc']/total_rbs:.1%})")
        
        # --- 3. Buffer ç‹€æ³ ---
        print(f"\nğŸ“¦ Buffer Status:")
        if obs is not None:
            embb_load_bits = obs[0]
            embb_hol_delay = obs[1]
            urllc_load_bits = obs[2]
            urllc_hol_delay = obs[3]
            
            print(f"   eMBB Buffer:  {embb_load_bits:,.0f} bits ({embb_load_bits/1e6:.2f} Mb)")
            print(f"   eMBB HoL Delay: {embb_hol_delay*1000:.3f} ms")
            print(f"   URLLC Buffer: {urllc_load_bits:,.0f} bits ({urllc_load_bits/1e6:.2f} Mb)")
            print(f"   URLLC HoL Delay: {urllc_hol_delay*1000:.3f} ms")
        
        # --- 4. é€šé“å“è³ª (CQI) ---
        print(f"\nğŸ“¡ Channel Quality (CQI):")
        if obs is not None:
            cqi_embb = obs[4]
            cqi_urllc = obs[5]
            print(f"   eMBB CQI:  {cqi_embb:.1f} / 15")
            print(f"   URLLC CQI: {cqi_urllc:.1f} / 15")
        
        # --- 5. æ•ˆèƒ½æŒ‡æ¨™ ---
        print(f"\nâš¡ Performance Metrics:")
        if 'throughput_embb_mbps' in info:
            print(f"   eMBB Throughput:  {info['throughput_embb_mbps']:.2f} Mbps")
        if 'throughput_urllc_mbps' in info:
            print(f"   URLLC Throughput: {info['throughput_urllc_mbps']:.2f} Mbps")
        if 'latency_urllc' in info:
            print(f"   URLLC Latency: {info['latency_urllc']*1000:.3f} ms")
        if 'dropped_urllc' in info:
            print(f"   URLLC Dropped: {info['dropped_urllc']} packets")
        
        # --- 6. çå‹µç´°é … ---
        reward = self.locals.get('rewards', [0])[0]
        print(f"\nğŸ’° Reward Breakdown:")
        print(f"   Total Reward: {reward:.4f}")
        if 'reward_throughput' in info:
            print(f"   + Throughput Reward: {info['reward_throughput']:.4f}")
        if 'reward_latency' in info:
            print(f"   - Latency Penalty:   {info['reward_latency']:.4f}")
        if 'reward_drop' in info:
            print(f"   - Drop Penalty:      {info['reward_drop']:.4f}")
        
        print("="*80 + "\n")

def main():
    # ==========================================
    # 1. åˆå§‹è¨­å®š (Setup)
    # ==========================================
    print("--- 1. Loading Configuration ---")
    config = load_config()
    
    # å»ºç«‹å¯¦é©— ID (åŠ ä¸Šæ™‚é–“æˆ³è¨˜ï¼Œé¿å…è¦†è“‹èˆŠå¯¦é©—)
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    exp_name = f"{config['experiment_name']}_{timestamp}"
    
    # è¨­å®šè·¯å¾‘
    log_dir = os.path.join(config['logging']['log_dir'], exp_name)
    save_dir = os.path.join(config['logging']['save_dir'], exp_name)
    
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"Experiment: {exp_name}")
    print(f"Logs: {log_dir}")
    print(f"Models: {save_dir}")

    # ==========================================
    # 2. å»ºç«‹ç’°å¢ƒ (Environment)
    # ==========================================
    print("\n--- 2. Creating Environment ---")
    
    # å»ºç«‹è¨“ç·´ç’°å¢ƒ (Training Env)
    # ä½¿ç”¨å¤šç’°å¢ƒä¸¦è¡Œ (SubprocVecEnv)
    def make_env(rank: int):
        def _init():
            env = NetworkSlicingEnv(config)
            env.reset(seed=config['random_seed'] + rank)
            return env
        return _init

    num_envs = int(config['agent'].get('num_envs', 1))
    env_fns = [make_env(i) for i in range(num_envs)]
    train_env = SubprocVecEnv(env_fns)
    train_env = VecMonitor(train_env, filename=os.path.join(log_dir, "train_monitor"))
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ (Evaluation Env)
    # ç¨ç«‹æ–¼è¨“ç·´ç’°å¢ƒï¼Œç”¨æ–¼ EvalCallback å®šæœŸæ¸¬è©¦æ¨¡å‹è¡¨ç¾ (ä¸å«å™ªè²)
    eval_env = NetworkSlicingEnv(config)
    eval_env = Monitor(eval_env, filename=os.path.join(log_dir, "eval_monitor"))

    # ==========================================
    # 3. å»ºç«‹å›èª¿å‡½æ•¸ (Callbacks)
    # ==========================================
    # è©•ä¼°å›èª¿ï¼šå®šæœŸæ¸¬è©¦æ¨¡å‹ä¸¦å„²å­˜æœ€ä½³æ¨¡å‹
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=save_dir,
        log_path=log_dir,
        eval_freq=config['eval_freq'],
        n_eval_episodes=config['n_eval_episodes'],
        deterministic=True, # æ¸¬è©¦æ™‚ä¸ä½¿ç”¨éš¨æ©Ÿæ¢ç´¢ï¼Œè©•ä¼°çœŸå¯¦å¯¦åŠ›
        render=False
    )
    
    # è©³ç´°æ—¥èªŒå›èª¿ï¼šé¡¯ç¤ºè¨“ç·´éç¨‹ä¸­çš„è©³ç´°è³‡è¨Š
    # log_freq å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´ (æ¯ 1000 æ­¥é¡¯ç¤ºä¸€æ¬¡)
    detailed_logging_callback = DetailedLoggingCallback(
        log_freq=1000,  # æ¯ 100 timesteps é¡¯ç¤ºä¸€æ¬¡è©³ç´°è³‡è¨Š
        verbose=1
    )
    
    # çµ„åˆå¤šå€‹ callbacks
    callbacks = CallbackList([eval_callback, detailed_logging_callback])

    # ==========================================
    # 4. åˆå§‹åŒ– Agent (SAC)
    # ==========================================
    print("\n--- 3. Initializing SAC Agent ---")
    
    agent_params = config['agent']
    
    model = SAC(
        "MlpPolicy",          # ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥æ©Ÿ (MLP) ä½œç‚ºç¥ç¶“ç¶²è·¯æ¶æ§‹
        train_env,
        verbose=1,
        tensorboard_log=log_dir,
        # å¾ config è¼‰å…¥è¶…åƒæ•¸
        learning_rate=agent_params['learning_rate'],
        buffer_size=agent_params['buffer_size'],
        batch_size=agent_params['batch_size'],
        gamma=agent_params['gamma'],
        tau=agent_params['tau'],
        ent_coef=agent_params['ent_coef'],
        seed=config['random_seed']
    )
    
    print(model.policy) # å°å‡ºç¶²è·¯æ¶æ§‹ç¢ºèª

    # ==========================================
    # 5. é–‹å§‹è¨“ç·´ (Training)
    # ==========================================
    print(f"\n--- 4. Starting Training for {config['total_timesteps']} steps ---")
    
    start_time = time.time()
    
    model.learn(
        total_timesteps=config['total_timesteps'],
        callback=callbacks,  # ä½¿ç”¨çµ„åˆçš„ callbacks
        progress_bar=True # é¡¯ç¤ºé€²åº¦æ¢
    )
    
    end_time = time.time()
    duration = end_time - start_time
    print(f"\n--- Training Finished in {duration:.2f} seconds ---")

    # ==========================================
    # 6. å­˜æª” (Saving)
    # ==========================================
    # å„²å­˜æœ€çµ‚æ¨¡å‹ (ä¸ä¸€å®šæ˜¯æœ€å¥½çš„ï¼Œä½†åŒ…å«äº†æœ€å¾Œçš„è¨“ç·´ç‹€æ…‹)
    final_path = os.path.join(save_dir, "final_model")
    model.save(final_path)
    print(f"Final model saved to: {final_path}.zip")
    
    # å¦å¤–å„²å­˜ä¸€ä»½ config å‚™ä»½ï¼Œæ–¹ä¾¿æœªä¾†æŸ¥é–±ç•¶æ™‚æ˜¯ç”¨ä»€éº¼åƒæ•¸è·‘çš„
    with open(os.path.join(save_dir, "config.yaml"), 'w') as f:
        yaml.dump(config, f)
        
    print("Done.")

if __name__ == "__main__":
    main()